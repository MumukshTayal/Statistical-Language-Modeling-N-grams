{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stastical Language Modelling Using N-grams",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "64DKvXvFX8n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOS = \"<s> \"\n",
        "EOS = \"</s>\"\n",
        "UNK = \"<UNK>\""
      ],
      "metadata": {
        "id": "UamOxAsYYA6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Sentence Tokens:\n",
        "    \n",
        "    To identify the beginning and end of the sentence \n",
        "    add the StartOfSentence and EndOfSentence tokens.\n",
        "\n",
        "    The argument 'sentences' takes a list of str and 'n' is the order of the model.\n",
        "    The function returns the list of generated of sentences.\n",
        "    \n",
        "    For bigram models (or greater) both tokens are added otherwise or only one is added."
      ],
      "metadata": {
        "id": "-kYJ7m7Wi1UU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence_tokens(sentences, n):\n",
        "    sos = SOS * (n-1) if n > 1 else SOS\n",
        "    return ['{}{} {}'.format(sos, s, EOS) for s in sentences]\n"
      ],
      "metadata": {
        "id": "gggphWNRYEQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replace singletons:\n",
        "    \n",
        "    For the tokens appearing only ones in the corpus, replace it with <UNK>\n",
        "    \n",
        "    The argument 'tokens' takes input of the tokens comprised in the corpus.\n",
        "    The function returns list of tokens after replacing each singleton with <UNK>\n",
        "\n",
        "    It is done to tackle the words which are not present in the vocabulary of the corpus (i.e., Out of Vocabulary words). \n",
        "    It assignes <UNK> token to the words which appeared only one time. \n",
        "    Then if new word is encountered, it is treated, its probility is assigned as the probability of <UNK> token."
      ],
      "metadata": {
        "id": "FMaiatvfkOte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_singletons(tokens):\n",
        "    vocab = nltk.FreqDist(tokens)       \n",
        "    \"\"\"FreqDist() returns dictionary with key as tokens and values as corresponding frequency of the token\"\"\"\n",
        "    return [token if vocab[token] > 1 else UNK for token in tokens]\n"
      ],
      "metadata": {
        "id": "CWOzdxIFYGOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess:\n",
        "    \n",
        "    The function takes the argument 'sentences' that takes the list of str of\n",
        "    preprocess. The argument 'n' is the order of the model.\n",
        "    Adds the above three tokens to the sentences and tokenize.\n",
        "    The function returns preprocessed sentences."
      ],
      "metadata": {
        "id": "f922mdUYknqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sentences, n):\n",
        "    sentences = add_sentence_tokens(sentences, n)\n",
        "    tokens = ' '.join(sentences).split(' ')\n",
        "    tokens = replace_singletons(tokens)\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "j6sYeVP6YIAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZhIn2T1pMRYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from itertools import product\n",
        "import math\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "_wGITV-EYKlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "    This function loads training and testing corpus from a directory.\n",
        "    The argument 'data_dir' contains path of the directory. The directory should contain files: 'train.txt' and 'test.txt'\n",
        "    Function returns train and test sets as lists of sentences."
      ],
      "metadata": {
        "id": "SvNv4CVcvSRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_dir):\n",
        "    train_path = data_dir + 'train.txt'\n",
        "    test_path  = data_dir + 'test.txt'\n",
        "\n",
        "    with open(train_path, 'r') as f:\n",
        "        train = [l.strip() for l in f.readlines()]\n",
        "    with open(test_path, 'r') as f:\n",
        "        test = [l.strip() for l in f.readlines()]\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "YDfSR0oKYPgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trained N-gram model:\n",
        "\n",
        "    A trained model for the given corpus is constructed by preprocessing the \n",
        "    corpus and calculating the smoothed probabilities of each n-gram. \n",
        "    The arguments contains training data (list of strings), n (integer; order of the model), \n",
        "    and an integer used for laplace smoothing.\n",
        "    Further, the model has a method for calculating perplexity."
      ],
      "metadata": {
        "id": "uChsuXQYvITI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(object):\n",
        "    def __init__(self, train_data, n, laplace=1):\n",
        "        self.n = n\n",
        "        self.laplace = laplace\n",
        "        self.tokens = preprocess(train_data, n)\n",
        "        self.vocab  = nltk.FreqDist(self.tokens)\n",
        "        self.model  = self._create_model()\n",
        "        self.masks  = list(reversed(list(product((0,1), repeat=n))))\n",
        "\n",
        "    def _smooth(self):\n",
        "        \"\"\"\n",
        "        The n tokens of n-gram in training corpus and first n-1 tokens of each n-gram\n",
        "        results in Laplace smoothenedd probability.\n",
        "        The function returns the smoothened probability mapped to its n-gram.\n",
        "\n",
        "        \"\"\"\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        n_grams = nltk.ngrams(self.tokens, self.n)\n",
        "        n_vocab = nltk.FreqDist(n_grams)\n",
        "\n",
        "        m_grams = nltk.ngrams(self.tokens, self.n-1)\n",
        "        m_vocab = nltk.FreqDist(m_grams)\n",
        "\n",
        "        def smoothed_count(n_gram, n_count):\n",
        "            m_gram = n_gram[:-1]\n",
        "            m_count = m_vocab[m_gram]\n",
        "            return (n_count + self.laplace) / (m_count + self.laplace * vocab_size)\n",
        "\n",
        "        return { n_gram: smoothed_count(n_gram, count) for n_gram, count in n_vocab.items() }\n",
        "\n",
        "    def _create_model(self):\n",
        "        \"\"\"\n",
        "        This function creates a probability distribution of the vocabulary of training corpus.\n",
        "        The probabilities in a unigram model are simply relative frequencies of each token over the whole corpus.\n",
        "        Otherwise, the relative frequencies are Laplace-smoothed probabilities.\n",
        "        Function returns a dictionary which maps each n-gram, which is in the form of tuple of strings, to its probabilities (float)\n",
        "\n",
        "        \"\"\"\n",
        "        if self.n == 1:\n",
        "            num_tokens = len(self.tokens)\n",
        "            return { (unigram,): count / num_tokens for unigram, count in self.vocab.items() }\n",
        "        else:\n",
        "            return self._smooth()\n",
        "\n",
        "    def _convert_oov(self, ngram):\n",
        "        \"\"\"\n",
        "        This function handles the words which are encountered in the test and converts the given n-gram to one which is known by the model.\n",
        "        Stop when the model contains an entry for every permutation.\n",
        "        The function returns n-gram with <UNK> tokens in certain positions such that the model\n",
        "            contains an entry for it.\n",
        "        \"\"\"\n",
        "        mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
        "\n",
        "        ngram = (ngram,) if type(ngram) is str else ngram\n",
        "        for possible_known in [mask(ngram, bitmask) for bitmask in self.masks]:\n",
        "            if possible_known in self.model:\n",
        "                return possible_known\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        \"\"\"\n",
        "        Perplexity of the model is calculated using the sentences and returns\n",
        "        a float value. \n",
        "        \n",
        "        \"\"\"\n",
        "        test_tokens = preprocess(test_data, self.n)\n",
        "        test_ngrams = nltk.ngrams(test_tokens, self.n)\n",
        "        N = len(test_tokens)\n",
        "\n",
        "        known_ngrams  = (self._convert_oov(ngram) for ngram in test_ngrams)\n",
        "        probabilities = [self.model[ngram] for ngram in known_ngrams]\n",
        "\n",
        "        return math.exp((-1/N) * sum(map(math.log, probabilities)))\n",
        "\n",
        "    def _best_candidate(self, prev, i, without=[]):\n",
        "        \"\"\"\n",
        "        Selects the most probable token depending on the basis of previous\n",
        "        (n-1) tokens. \n",
        "        The function takes the argument of previous (n-1) tokens, and the tokens to\n",
        "        exclude from candidates list.\n",
        "        The function returns the most probable token and its probability.\n",
        "\n",
        "        \"\"\"\n",
        "        blacklist  = [\"<UNK>\"] + without\n",
        "        candidates = ((ngram[-1],prob) for ngram,prob in self.model.items() if ngram[:-1]==prev)\n",
        "        candidates = filter(lambda candidate: candidate[0] not in blacklist, candidates)\n",
        "        candidates = sorted(candidates, key=lambda candidate: candidate[1], reverse=True)\n",
        "        if len(candidates) == 0:\n",
        "            return (\"</s>\", 1)\n",
        "        else:\n",
        "            return candidates[0 if prev != () and prev[-1] != \"<s>\" else i]\n",
        "     \n",
        "    "
      ],
      "metadata": {
        "id": "zCPcyYs-YfQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/Shareddrives/MathProject22/Dataset/data/'\n",
        "train, test = load_data(data_path)"
      ],
      "metadata": {
        "id": "nvUu5JuPqibM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#if __name__ == '__main__':\n",
        "model_instance= LanguageModel(train[0:1000000], 3, 0)\n",
        "   # first number is the n of n gram\n",
        "   # second number is the coefficient whether laplace used or not "
      ],
      "metadata": {
        "id": "bn6mfiDabROJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_instance.perplexity(test))"
      ],
      "metadata": {
        "id": "XPFkyVXh31LT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba642fe-a46a-451e-d380-4e38afad2c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.3419976141234033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev=('I','love',)\n",
        "print(model_instance._best_candidate(prev,1)[0])\n",
        "# `1 is ith best fit as a candidate\n"
      ],
      "metadata": {
        "id": "0fJ4Oz3H7piu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70ea043-fd71-4c94-8e77-fa372832ff6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = 'without_laplace.sav'\n",
        "pickle.dump(model_instance, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "hAHYM7MJCGXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train)"
      ],
      "metadata": {
        "id": "JPrRStcCCmS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3f2ffdd-7744-42fa-8e55-c8842cd51641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7871825"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    }
  ]
}